% !Mode:: "TeX:UTF-8"
\def\usewhat{pdflatex}                               % 定义编译方式 dvipdfmx 或者 pdflatex，默认为 dvipdfmx
                                                     % 方式编译，如果需要修改，只需改变花括号中的内容即可。
%\documentclass[12pt,openany,oneside]{book}

\documentclass[a4paper, 11pt]{article}                                                   % 本科生毕业论文通常采用单页排版
\usepackage{pdfpages}
\usepackage{float}
\usepackage{wrapfig}

\usepackage[nottoc]{tocbibind}
%\usepackage{indentfirst}

%\usepackage{graphicx}
%\usepackage{caption}
%\usepackage{subcaption}
%\usepackage{subfigure}

%\makeatletter
%\let\c@subfigure\relax
%\let\l@subfigure\relax
%\makeatother

%\usepackage{epstopdf}


\input{setup/package}                                % 定义本文所使用宏包
\graphicspath{{figures/}}                            % 定义所有的 .eps 文件在 figures 子目录下
\begin{document}                                     % 开始全文

\begin{CJK*}{UTF8}{song}
\begin{center}

  \begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/ustc}
  \end{figure}

\vspace*{25pt}
{ \fontsize{20pt}{24pt}\selectfont{\textbf{“大学生创新训练计划”项目结题报告}}}

\vspace*{20pt}

\begin{center}
  \begin{tabular}{lclc}
    中文题目 &  \underline{基于SDN的云中大数据网络优化的研究} \\
    英\qquad 文& \underline{ Research of bigdata network optimization} \\
    题\qquad 目& \underline{based on SDN in cloud computing} \\
    学\qquad 院& \underline{信息学院} \\
    指导教师 &  \underline{张文逸} \\
    \end{tabular}
    \vspace*{50pt}
    \begin{tabular}{lclc}
    小组成员： \\
    姓\qquad 名& \underline{孟思尧} & 学\qquad 号 & \underline{PB12210076} \\
    姓\qquad 名& \underline{王笑霄} & 学\qquad 号 & \underline{PB12210100} \\
    姓\qquad 名& \underline{张广帅}  & 学\qquad 号 & \underline{PB12210080} \\
  \end{tabular}
\end{center}

  \vspace*{25pt}
  \today

\end{center}
\clearpage
\begin{center}
  { \fontsize{15pt}{18pt}\selectfont{\textbf{摘\qquad 要}}}
\end{center}

由于当前数据中心用并行计算的框架处理大规模的数据，根据调查发现网络是限制大数据处理性能的一大瓶颈。针对网络问题，我们引入了软件定义网络，并针对开源的 Apache Hadoop MapReduce 平台实现了一个可行的调度控制系统，并将 Coflow 的概念引入了大数据平台，建立了控制架构，实现了两个Coflow的调度算法。仿真结果显示我们设计的最小化Coflow完成时间的调度算法要比先到达先服务的调度算法节省10倍左右的Coflow完成时间。进一步我们尝试通过一系列工业优化方法克服 SDN 中高带宽 TCP 应用的难点问题，如包超时重传导致的拥堵以及难以直接从 Hadoop MapReduce 模块中拿到调度流所必须的信息等等。 测试结果表明，在使用我们的调度器后，MapReduce 任务的性能在 SDN 环境下提升了 5\%。

{ \fontsize{10pt}{15pt}\selectfont{\textbf{关键词：}}}云计算，大数据，SDN，HADOOP，网络优化
\clearpage
\begin{center}
  { \fontsize{15pt}{18pt}\selectfont{\textbf{ABSTRACT}}}
\end{center}

Parallel processing in Big Data is a trend nowadays. And network is a bottleneck for the system overall performance. To study this problem, we introduced the Software Defined Network solution to the Big Data Network and developed a viable scheduling system based on Open-Source Apache Hadoop MapReduce platform. Also, we introduced the concept of Coflow into the system to boost the overall performance and implemented two scheduling algorithms. The simulation result shows that the Coflow completion time is 10x faster than the that without Coflow policy. Furthermore, we use a series of industrial optimization methods to overcome the difficulties of using SDN in traditional TCP networks, such as TCP retransmission-caused congestion and inconvenience in taking information necessary for flow scheduling directly from Hadoop MapReduce. The result shows than our scheduler has a 5\% performance gain over the original system.

{ \fontsize{10pt}{15pt}\selectfont{\textbf{KEY WORDS：}}}Cloud computing，Bigdata，SDN，HADOOP，network optimization
\clearpage
\tableofcontents
%\part{问题阐述}
%\chapter{问题阐述}
\section{项目背景}
\subsection{项目的意义和关键问题}
大数据应用正深刻改变人们的生活，已经成为当前学术界、工业界的关注焦点。大数据应用开发云平台纷纷推出大数据计算框架。如Amazon的EC2，微软的Azure。虽然云平台上的大数据框架方便了第三方开发者和用户，但其性能上还存在较多问题，尤其是网络的问题。在2014 Daytona GraySort排序赛上，基于Spark的系统它使用了207个EC2 节点在23分钟内排序了100TB的数据而夺冠。而上届冠军Hadoop用了2100台Yahoo内置的机器，花了72分钟，这性能提升不言而喻！更重要的是这次比赛证实Shuffle真正的瓶颈在于网络。传统大数据网络架构是数据库服务器将应用服务器请求的数据通过网络传输到应用服务器上，处理后，将改写的数据再写回数据库服务器，而且处理过程中会出现大量的中间结果也需要网络传输。当数据量较大时，很可能堵塞网络。因而对于进一步提升大数据处理性能，研究云平台上大数据网络问题显得尤为重要。\\
\subsection{研究方向}
\begin{description}
  \item[软件定义网络（SDN）] 软件定义网络（SDN）是当前和未来网络研究的一个重要方向，通过将网络的数据转发层和逻辑控制层分离，提高了网络的灵活性和可编程性。可以在控制层面设计应用感知的控制策略，如网络接入、数据转发路由、流量工程等，从而提高应用的性能。因此，我们拟通过SDN来优化云平台上大数据框架的网络性能。
  \item[coflow调度] 考虑到当前数据中心用并行计算的框架处理大规模的数据，充分考虑数据流的相关性（coflow），设计调度算法
\end{description}
\section{项目平台的选择}
\subsection{Hadoop}
\subsubsection{为什么选择Hadoop}
数据并行计算框架很多，如：Mapreduce，Spark，Google Dataflow等等，我们为什么选择Hadoop？
首先，认识一下Hadoop，Hadoop是一种计算集群，它将数据分析的工作分配到多个集群节点上，从而并行处理数据。经过调研，Hadoop用于大数据处理，主要有如下几个优势：
\begin{description}
  \item[1、灵活的可扩展性] 要充分利用大数据最大的优势就需要实时或接近实时地对海量数据进行分析处理。大数据分析面临的一个巨大的难题是数据量的不断增加。而Hadoop集群的并行处理能力能明显提高分析速度，但随着要分析的数据量的增加，集群的处理能力会受到影响。Hadoop通过增加集群节点，可以线性地扩展集群以处理更大的数据集。另外，在集群负载下降时，也可以减少节点，以高效使用计算资源。所以Hadoop的弹性很好
  \item[2、Hadoop的设计适合大数据处理] 大数据一般都是分布广泛的并且是非结构化的。而Hadoop非常适合处理这类数据，因为Hadoop的mapreduce的计算框架工作原理是将数据拆分成片，并将每个“分片”分配到特定的集群节点上进行分析。数据不必均匀分布，因为每个数据分片都是在独立的集群节点上进行单独处理。
  \item[3、Hadoop成本低] Hadoop的软件是开源的，同时，Hadoop支持商用硬件，可以运行在一般商业机器构成的大型集群上，如：亚马逊弹性计算云（Amazon EC2）等，不必花费重金购买服务器级别的硬件设备。所以我们可以低成本的实现计算能力强大的Hadoop集群，性价比很高。
  \item[4、容错能力强] 在Hadoop集群进行大数据处理分析过程中，当一个数据分片发送到某个节点进行分析时，该数据在集群其它节点上会存有副本。通过备份的方式，即使一个节点发生故障，数据可以快速的恢复，继续进行分析处理。故障检测和自动恢复是Hadoop最初的设计目标，所以Hadoop很健壮。
\end{description}
由于Hadoop具有上述优势，使得Hadoop在学术界和工业界都大受欢迎。\\
\subsubsection{Mapreduce计算框架}
Mapreduce是什么？怎样完成大数据处理？\\
Hadoop的设计思路源于Google的GFS和MapReduce。它是一个开源软件框架，通过在集群计算机中使用MapReduce这个简单的编程模型，可编写和运行分布式应用程序处理大规模数据。

\begin{figure}[!htb]
    \centering
    \let\c@\includegraphics[height=2in]{mapreduce.png}\relax
    \caption{Mapreduce计算框架}
\end{figure}
MapReduce架构的大数据处理的操作流程如下：
\begin{enumerate}
  \item 首先调用MapReduce库的输入流模块，将输入文件分成M个数据片度，每个数据片段的大小可以通过可选的参数来控制。然后用户程序在机群中创建大量的程序副本。程序副本中包含一个特殊的master程序。其它的程序都是worker程序，由master分配任务。有M个Map任务和R个Reduce任务将被分配，master将一个Map任务或Reduce任务分配给一个空闲的worker。
  \item 被分配了map任务的worker程序读取相关的输入数据片段，从输入的数据片段中解析出key/value pair，然后把key/value pair传递给用户自定义的Map函数，由Map函数生成并输出的中间key/value pair，并缓存在内存中。
  \item 缓存中的key/value pair通过调用Partition模块分成R个区域，之后周期性的写入到本地磁盘上。缓存的key/value pair 在本地磁盘上的存储位置将被传给master，由master负责把这些存储位置再传送给Reduce worker。
  \item 当Reduce worker程序接收到master程序发来的数据存储位置信息后，使用RPC从Map worker所在主机的磁盘上读取这些缓存数据。当Reduce worker 读取了所有的中间数据后，通过对key进行排序后使得具有相同key值的数据聚合在一起。由于许多不同的key值会映射到相同的Reduce 任务上，因此必须进行排序。如果中间数据太大无法在内存中完成排序，那么就要在外部进行排序。
  \item Reduce worker程序遍历排序后的中间数据，对于每一个唯一的中间key值，Reduce worker程序将这个key值和它相关的中间value值的集合传递给用户自定义的Reduce函数。Reduce函数的输出被追加到所属分区的输出文件。
  \item 当所有的Map和Reduce任务都完成之后，master唤醒用户程序。在这个时候，在用户程序里的对MapReduce调用才返回。在成功完成任务之后，MapReduce的输出存放在R个输出文件中（对应每个Reduce任务产生一个输出文件，文件名由用户指定）。一般情况下，用户不需要将这R个输出文件合并成一个文件–他们经常把这些文件作为另外一个MapReduce的输入，或者在另外一个可以处理多个分割文件的分布式应用中使用。
\end{enumerate}

\subsubsection{Shuffle阶段}
之前我们说shuffle阶段是制约大数据处理的网络瓶颈，下面我们将对Mapreduce的shuffle阶段进行详细的介绍。整体的Shuffle过程包含以下几个部分：Map 端Shuffle、Sort 阶段、Reduce 端Shuffle。 即是说：Shuffle 过程横跨 map 和 reduce 两端，中间包含 sort 阶段。

\begin{figure}[!htb]
    \centering
    \let\c@\includegraphics[height=2in]{shuffle.png}\relax
    \caption{shuffle阶段}
\end{figure}
 在Hadoop集群中，大部分map task与reduce task的执行是在不同的节点上，Reducer通过HTTP方式得到map阶段输出文件的分区，所以很多情况下Reduce 执行时需要跨节点去拉取其它节点上的map task结果。在Mapreduce的Shuffle阶段，如果集群正在运行的 job 有很多，而且需要跨节点拉取数据时，会产生大量的数据在网络中传输，对集群内部的网络资源消耗会很严重，尽可能地减少对带宽的不必要消耗，并且保证完整地从map task 端拉取数据到reduce端。
\subsection{SDN}
\subsubsection{Floodlight}
SDN（软件定义网络）利用OpenFlow协议，把路由器的控制平面（control plane）从数据平面（data plane）中分离出来，以软件方式实现。这个架构可以让网络管理员，在不改动硬件设备的前提下，通过集中式的控制器（Controller）以标准化的接口对各种网络设备进行管理和配置，那么这将为网络资源的设计、管理和使用提供更多的可能性，为控制网络流量提供了新的方法。\\
控制器作为SDN网络中的重要组成部分，我们选择Floodlight作为SDN控制器。因为Floodlight是目前主流的SDN控制器之一，它的稳定性、易用性已经得到SDN 专业人士一致好评，由于其完全开源，这让SDN网络世界变得更加有活力。能集中地灵活控制SDN网络，为核心网络及应用创新提供了良好的扩展平台。\\
Floodlight控制器是一个企业级的，使用Java开发的OpenFlow协议的控制器。OpenFlow是一个由Open Networking Foundation (ONF)管理的开放标准。它定义了一种协议让远程控制器通过路由器可以修改网络设备的行为，使用定义良好的转发指令集。Floodlight被设计为同支持OpenFlow 标准的设备（交换机，路由器，虚拟交换机）一起工作。\\
Floodlight controller模块为多数应用实现了一些通用的功能：

\begin{enumerate}
  \item 发现网络状态和事件（拓扑结构，设备，流量）
  \item 能够控制网络交换机（network switches）通信
  \item 管理floodlight模块，共享存储，线程，测试等资源
  \item 提供一个web界面和debug服务器（Python）
\end{enumerate}
\begin{figure}[!htb]
    \centering
    \let\c@\includegraphics[height=2.5in]{project_floodlight_infographic.png}\relax
    \caption{Floodlight控制器架构图}
\end{figure}
Floodlight控制器工作过程如下：
\begin{enumerate}
  \item 控制器与交换机建立ofchannel通道，控制器通过ofchannel 控制和管理交换机。
  \item 当交换机收到一个数据包且流表中没有匹配条目，交换机会将数据包封装在packet-in消息发送给控制器，此时数据包会缓存在交换机中等待处理。
  \item 控制器收到packet-in消息后，可以发送flow-mod消息向交换机写一个流表项，并且将flow-mod消息中buffer-id字段设置为packet-in 消息中的buffer-id值。从而控制器向交换机写入了一条与数据包相关的流表项，并且指定该数据包按照该流表项的action列表处理。但是并不是所有的数据包都需要向交换机中添加一条流表项来匹配处理，网络中还存在多种数据包，它出现的数量很少（如ARP,IGMP 等），以至于没有必要通过流表项来指定这一类数据包的处理法，此时控制器可以使用packet-out消息，告诉交换机某一个数据包如何处理。
\end{enumerate}
\subsubsection{OVS}
Open vSwitch是一个由Nicira Networks主导的开源项目，通过运行在虚拟化平台上的虚拟交换机，为本台物理机上的VM提供二层网络接入， 跟云中的其它物理交换机一样工作在Layer 2层。Open vSwitch充分考虑了在不同虚拟化平台间的移植性，采用平台无关的C语言开发。并且遵循Apache2.0许可；我们有传统的物理交换机，为什么还要开发Open vSwitch呢？\\
在传统数据中心中，网络管理员习惯了每台物理机的网络接入均可见并且可配置。通过在交换机某端口的策略配置，可以很好控制指定物理机的网络接入，访问策略，网络隔离，流量监控，数据包分析，Qos 配置，流量优化等。但是在云平台上，如果没有网络虚拟化技术的支持，管理员只能看到被桥接的物理网卡，其上川流不息地跑着n台VM 的数据包。仅凭物理交换机支持,管理员无法区分这些包属于哪个OS 哪个用户。而且难以满足以下需求：
\begin{description}
  \item[网络隔离] 物理网络管理员早已习惯了把不同的用户组放在不同的VLAN中，例如研发部门、销售部门、财务部门，做到二层网络隔离。Open vSwitch通过在host上虚拟出一个软件交换机，等于在物理交换机上级联了一台新的交换机，所有VM通过级联交换机接入，让管理员能够像配置物理交换机一样把同一台host上的众多VM分配到不同VLAN 中去；
  \item[QoS配置] 在共享同一个物理网卡的众多VM中，我们期望给每台VM 配置不同的速度和带宽，以保证核心业务VM的网络性能。通过在Open vSwitch端口上，给各个VM配置QoS，可以实现物理交换机的traffic queuing和traffic shaping功能。
  \item[流量监控] 物理交换机通过xxFlow技术对数据包采样，记录关键域，发往Analyzer处理。进而实现包括网络监控、应用软件监控、用户监控、网络规划、安全分析、会计和结算、以及网络流量数据库分析和挖掘在内的各项操作。例如，NetFlow流量统计可以采集的数据非常丰富，包括：数据流时戳、源IP地址和目的IP地址、 源端口号和目的端口号、输入接口号和输出接口号、下一跳IP地址、信息流中的总字节数、信息流中的数据包数量、信息流中的第一个和最后一个数据包时戳、源AS和目的AS，及前置掩码序号等。\\
      xxFlow因其方便、快捷、动态、高效的特点，为越来越多的网管人员所接受，成为互联网安全管理的重要手段，特别是在较大网络的管理中，更能体现出其独特优势。有了Open vSwitch，作为网管的你，可以把xxFlow的强大淋漓尽致地应用在VM上！
  \item[数据包分析] 物理交换机的一大卖点，当对某一端口的数据包感兴趣时（for trouble shooting , etc），可以配置各种span（SPAN, RSPAN, ERSPAN），把该端口的数据包复制转发到指定端口，通过抓包工具进行分析。Open vSwitch官网列出了对SPAN, RSPAN, and GRE-tunneled mirrors 的支持。
\end{description}
Open vSwitch 引入了以下模块，很好地满足以上需求：
\begin{enumerate}
  \item  ovs-openflowd  ---  OpenFlow交换机；
  \item  ovs-controller  --- OpenFlow控制器；
  \item  ovs-ofctl  --- Open Flow 的命令行配置接口；
  \item  ovs-pki  --- 创建和管理公钥框架；
  \item  tcpdump的补丁 --- 解析OpenFlow的消息；
\end{enumerate}

\begin{figure}[ht]
    \centering
    \let\c@\includegraphics[height=3.5in]{ovs_work_procedure.png}\relax
    \caption{OVS工作流图}
\end{figure}

当Open vSwitch的一个接口收到数据包后，会按照上述流程图处理：收到数据包后，会交给datapath内核模块处理，当匹配到对应的datapath 会直接输出，如果没有匹配到，会交给用户态的ovs-vswitchd查询flow，用户态处理后，会把处理完的数据包输出到正确的端口，并且设置新的datapath规则，后续数据包可以通过新的datapath规则实现快速转发。

\section{基于Coflow的大数据网络优化}
\subsection{coflow相关知识介绍}
考虑到当前数据中心用并行计算的框架处理大规模的数据。许多数据密集型的应用都是和网络绑定的，但是对一些特别的通信请求，网络层的优化仍然是不可知道的，这通常会影响到应用层的表现。尽管数据密集型应用的架构有所不同，但是他们的通信相似甚至相同的，都要经过集群之间一系列的连续的计算过程。通常一个通信阶段的所有的流都完成这个通信阶段才算完成。最近提出的coflow的概念就代表一组满足相同的通信请求（最小化完成时间、所有的流满足deadline）并行流的集合，这个可以通过应用感知的网络调度算法实现。更进一步，我们可以通过优化coflow的完成时间整个任务的响应时间。\\
为什么说考虑coflow级别的调度算法有利于优化任务的完成时间呢？我们看下面这个例子：

\begin{figure}[!htb]
    \centering
    \let\c@\includegraphics[height=2in]{coflow_cct.png}\relax
    \caption{coflow完成时间的比较}
    \label{fig:coflow_cct}
\end{figure}
在本例中，Coflow1用黑色表示，含有一条流，需要1个单位时间传输完成；Coflow2用黄色表示，含有两条流，一条流flow1需要6个单位时间传输完成，另一条流flow2需要2个单位时间传输完成。Coflow1的一条流和Coflow2的flow2公用资源。如果在流级别按照最短处理时间优先的调度算法（Figure 5 的左侧图），Coflow1的完成时间为5，Coflow2的完成时间为6；若我们采用Coflow级别的调度算法，按照Coflow的最短完成时间调度，我们会发现虽然Coflow2的完成时间仍然为6，但是Coflow1的完成时间变为了3，从而缩短了Coflow的平均完成时间。\\
有人设计了varys来提高大数据处理过程中网络通信的性能，他们以最小化Coflow的完成时间和满足deadline为目标实现了FIFO、SCF和SEBF等启发式算法。采用varys 仿真器coflowsim完成Facebook真实数据的调度测试。他们采用的是交换结构，如下图：

\begin{figure}[!htb]
    \centering
    \let\c@\includegraphics[height=2in]{varys_switch.png}\relax
    \caption{varys中的交换结构}
\end{figure}
上面是一个交换结构，考虑的是无阻塞的网络，也就是说：不管网络处于何种状态，任何时刻都可以在交换网络中建立一个连接，只要这个连接的起点、终点是空闲的，而不会影响网络中已建立起来的连接。对于数据中心网络来说，无阻塞就是实现任意服务器之间可以线速交互流量的方式，作为数据中心交换机，无阻塞可以支撑数据中心网络内部大量的内部流量，满足这些流量的高效传输。
\subsection{coflow调度算法设计}
varys是交换结构，考虑无阻塞的情况。我们在设计coflow调度算法考虑路由，在COflow级别我们考虑最小化Coflow的完成时间，在Coflow内部的flow级别，我们考虑最大流优先，最短路径路由。具体算法如下：

\begin{algorithm}[H]
    \caption{最小化coflow的完成时间的调度算法}
    \label{alg:min_cct_Algo} % 贴上标签以便交叉引用
    \begin{algorithmic}[1]  % 这个 1 表示每一行都显示数字
    \STATE 生成Coflow任务
    \FOR{$t=0;t\le M; t\rightarrow t + 1$}
        \IF{当前部署的任务流已经处理完成}
            \STATE 释放网络资源；
        \ENDIF
        \STATE 统计当前时刻到达的Coflow；
        \IF{有Coflow到达}
            \STATE 根据当前资源，调用流级别的调度算法MFF-SP（算法：\ref{alg:MFF_SP_Algo}），计算Coflow的完成时间；
            \STATE 根据Coflow的完成时间，进行升序排序；
            \STATE Coflow完成时间最短的先部署
            \STATE 更新网络资源
        \ELSE
            \STATE 进入下一个时刻
        \ENDIF
    \ENDFOR
    \end{algorithmic}
\end{algorithm}
\noindent\hrule
\vspace{0.1em}\noindent\hrule

\bigskip

\begin{algorithm}[H]
    \caption{流级别的调度算法——最大流优先最短路径路由（MFF-SP）}
    \label{alg:MFF_SP_Algo} % 贴上标签以便交叉引用
    \begin{algorithmic}[1]  % 这个 1 表示每一行都显示数字
    \STATE 调用当前网络拓扑
    \STATE 对Coflow中的所有flow按照flow数据量大小，降序排列；
    \FOR{$flow=0;flow\le flow_number; flow\rightarrow flow + 1$}
        \IF{flow正在等待调度}
            \STATE 根据flow的源和目的节点利用最短路径计算其路由路径；
            \STATE 找到路由路径中剩余带宽最少的link；
            \STATE 根据剩余带宽最少的link的带宽计算Coflow的最小完成时间；
        \ELSE
            \STATE 进行下一条流的计算
        \ENDIF
    \ENDFOR
    \end{algorithmic}
\end{algorithm}
\noindent\hrule
\vspace{0em}\noindent\hrule
\subsection{coflow调度算法的性能表现}
我们完成了两种Coflow调度算法的设计：
\begin{enumerate}
  \item 先来的先调度（FCFS: first come first scheduling）
  \item 最小化Coflow的完成时间（MCCTF: minimum cct (coflow completion time) first）
\end{enumerate}
针对两种算法我们跑了一些仿真结果：

%Coflow平均完成时间图
\begin{figure}[ht]
    \centering
    \let\c@\includegraphics[height=1.8in]{Coflow_cct_fcfs_rate_jobrate_0_5.jpg}\relax
    \let\c@\includegraphics[height=1.8in]{Coflow_cct_mccft_rate_jobrate_0_5.jpg}\relax

    \let\c@\includegraphics[height=1.8in]{Coflow_cct_fcfs_rate.jpg}\relax
    \let\c@\includegraphics[height=1.8in]{Coflow_cct_mccft_rate.jpg}\relax
    \caption{FCFS和MCCFT两种算法在job到达速度分别为0.5和2.5时Coflow完成时间对比}
\end{figure}
根据图（Figure 7）显示的结果，我们发现在我们调用Coflow调度算法FCFS 时，Coflow的完成时间分布在$0\backsim140$个单位时间之间，而在调用我们设计的Coflow 调度算法MCCFT 时，在15 个单位时间以内的Coflow 占据全部Coflow 的$95\%$ 以上。

\begin{figure}[ht]
    \centering
    \let\c@\includegraphics[height=1.8in]{bw_statistics_fcfs.jpg}\relax
    \let\c@\includegraphics[height=1.8in]{bw_statistics_mcctf.jpg}\relax
    \caption{FCFS和MCCFT两种算法的每条链路上的带宽利用率对比}
\end{figure}
根据图（Figure 8）显示的结果，我们发现Coflow调度算法MCCFT的链路带宽利用率要比FCFS高出10倍左右

%带宽利用率图
\begin{figure}[ht]
    \centering
    \let\c@\includegraphics[height=2in]{Coflow_meancct_twoalgo_jobrate_0_5.jpg}\relax
    \let\c@\includegraphics[height=2in]{bw_utilize_rate_twoalgo_jobrate_0_5.jpg}\relax

    \let\c@\includegraphics[height=2in]{Coflow_meancct_twoalgo_jobrate_2_5.jpg}\relax
    \let\c@\includegraphics[height=2in]{bw_utilize_rate_twoalgo_jobrate_2_5.jpg}\relax
    \caption{FCFS和MCCFT两种算法的在job到达速度分别为0.5和2.5时Coflow平均带宽利用率以及平均完成时间对比}
\end{figure}
根据图（Figure 9）中的显示结果，我们可以看到算法MCCTF的Coflow的完成时间明显比算法FCFS的完成时间短，说明我们设计的算法是有效果的，最小化Coflow 的完成时间的Coflow调度算法确实可以缩短任务的平均完成时间。同时也验证了我们之前对Coflow完成时间优化的分析，如Figure:\ref{fig:coflow_cct}。\\

综上三组对比图我们发现，我们设计的Coflow调度算法MCCFT相比传统的Flow 级别的调度算法更加优越，因为MCCFT可以实现更小的Coflow完成时间，更高的带宽利用率。而且我们发现越是数据密集型的数据，我们的算法优势越是明显。\\
\vspace{10em}
\section{实验平台搭建}
\subsection{实验环境}
\begin{figure}[!htb]
    \centering
    \let\c@\includegraphics[height=2.5in,angle=-90]{experiment_environment1.JPG}\relax
    \let\c@\includegraphics[height=2.5in,angle=-90]{experiment_environment2.JPG}\relax
    \caption{真实的实验环境}
\end{figure}

服务器的硬件以及软件的配置：机器的型号，内存大小，网卡，协议版本等等

\begin{table}[!htb]
\caption{系统软硬件及协议配置} \label{table:confg}
\centering
\begin{tabular}{|c|c|c|}
  \hline
  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
  名称 &  配置 \\
  \hline
  服务器配置   &  CPU：16 Intel(R) Xeon(R) CPU\\
             &    E5620 @ 2.40GHz\\
             &    MEM：64G\\
             &    DISK：1000G\\
             &  网卡: 1000 Mbps\\
  \hline
  软件及协议 & Ubuntu 14.04 LTS \\
             & Floodlight 1.2 Modified\\
             & Open vSwitch 2.02 \\
             & OpenFlow 1.3 \\
             & Hadoop 2.7.1    \\
             & Python 2.7.11    \\
  \hline
\end{tabular}
\end{table}
\subsection{Hadoop平台搭建}
我们使用了4台 Xeon E5620 64GB RAM 服务器作为我们的 YARN MapReduce 任务执行节点，并对 YARN、Mapreduce 以及 HDFS 进行了配置。

为了使得整个平台的数据流量只走我们的 SDN 网络，我们分别对如下文件进行了如下配置：

\begin{figure}[!htb]
    \centering
    \let\c@\includegraphics[height=1.5in]{profile_list_212.png}\relax
    \caption{以服务器212为例，配置文件清单}
\end{figure}
YARN 平台配置文件 yarn-site.xml\\
1.设置 YARN 默认允许使用的系统资源量，若不进行设置，则只能使用 16GB 内存；

\begin{figure}[!htb]
    \centering
    \let\c@\includegraphics[height=2in]{profile_yarn_resource.png}\relax
    \caption{yarn配置，允许使用的资源}
\end{figure}
2.固定 Node Manager 的地址为本机 OpenFlow 网段IP（of-IPL212）；

\begin{figure}[!htb]
    \centering
    \let\c@\includegraphics[height=2in]{profile_yarn_nodemanager.png}\relax
    \caption{yarn 配置，指定nodemanager地址}
\end{figure}
3.固定 Resource Manager 的地址为 Master 的 OpenFlow 网段IP（of-master）；

\begin{figure}[!htb]
    \centering
    \let\c@\includegraphics[height=3in]{profile_yarn_resourcemanager.png}\relax
    \caption{yarn配置 指定resourcemanager地址}
\end{figure}
\bigskip

MapReduce 配置 mapred-site.xml\\
设置单个 Map 和 Reduce 任务可以占用的内存大小，防止单个任务的所需内存不足或分块任务太多而导致整体速率下降；
\begin{figure}[!htb]
    \centering
    \let\c@\includegraphics[height=2in]{MR_yarn_manage_task.png}\relax
    \caption{MR设置使用yarn管理tasks，单个task占用内存大小}
\end{figure}
\bigskip

HDFS 配置 hdfs-site.xml\\
设置强制所有 HDFS 客户端只监听 OpenFlow 端的端口（默认是0.0.0.0全部监听），避免数据流量进入控制网络。
\begin{figure}[!htb]
    \centering
    \let\c@\includegraphics[height=2.5in]{HDFS_flowto_SDN.png}\relax
    \caption{HDFS强制流量走SDN网络}
\end{figure}
\bigskip

\vspace{10em}
Master-Slave 关系配置\\
在主节点 Master 上的 slaves 文件中加入所有服务器的列表。
\begin{figure}[!htb]
    \centering
    \let\c@\includegraphics[height=1.5in]{slaves.png}\relax
    \caption{slaves}
\end{figure}
\vspace{2em}
\subsection{SDN开发环境搭建}

交换机采用 Open vSwitch，运行在 Ubuntu Linux 下。控制器采用 Floodlight，获取和控制整个 SDN 网络，并且与我们的 PyCon 进行通信。

将多个虚拟交换机ovs挂载到了floodlight控制器上，同时保留一个被动监听端口以供 PyCon 进行额外的操作：\\
ovs-vsctl set-controller ovsbridgename tcp:floodlight-ip:6633 ptcp:6666
\bigskip

允许远程控制虚拟交换机控制器，以便 PyCon 控制：\\
ovs-vsctl set-manager ptcp:6640

\vspace{2em}

\begin{figure}[!htb]
    \centering
    \let\c@\includegraphics[height=3.5in]{FL_Web_UI.png}\relax
    \caption{Floodlight Web 监视界面}
\end{figure}

%利用ovs完成hadoop平台上的数据导流工作
\subsection{网络拓扑}
通过hadoop以及SDN开发环境的搭建，我们构造了如下图（Figure\ref{fig:topology}）

\begin{figure}[!htb]
    \centering
    \let\c@\includegraphics[height=3in]{topology.png}\relax
    \caption{完整详细的网络拓扑结构}
    \label{fig:topology}
\end{figure}

\section{系统设计与开发}
\subsection{系统整体架构设计}
\begin{figure}[!htb]
    \centering
    \let\c@\includegraphics[height=3in]{work_arch.jpg}\relax
    \caption{基于SDN+Hadoop的系统架构设计}
\end{figure}
%在下面详细描述一下系统架构图的运行流程，描述一下各个模块之间的工作流程
从系统架构图可以看到，PyCon 作为我们的中心调度器，与诸多组件有通信：
1. 在 PyCon 启动时，会调用 OVS 远程接口对各个 OVS 交换机进行队列的直接配置，以及初始流表的下发;
2. PyCon 的 HTTP Server 模块从 Hadoop 和 Floodlight 插件处获取 HTTP POST 信息，里面包含调度流时所必须的一些信息;
3. 当 HTTP Server 收到一个完整的流信息，便会执行算法模块，然后调用流表下发 API 进行铺路。
4. PyCon 内部另有一个定时模块，每秒从 Floodlight 获取实时的网络带宽信息。

OVS 交换机会将匹配 actions=controller 的包以 packet in 的形式上传到 Floodlight 控制器，然后我们编写的 Floodlight 插件会对这个包进行分析并将需要的信息发送给 PyCon。

\subsection{MapReduce Fetcher}
Map任务结束之后，主要进行Copy、Sort、Reduce三个步骤；其中Copy阶段，就是从执行各个Map任务的节点获取map的输出文件。这是由ReduceTask.ReduceCopier 类来负责。ReduceCopier对象负责将Map函数的输出拷贝至Reduce所在机器。在这个过程中，Reduce 进程会启动一些数据复制线程 (Fetcher)，通过 HTTP GET 方法请求由 HTTP Server 管理的 Shuffle 数据块，这些数据块是先前成功完成的 Map Task 的输出文件。这些等待 Shuffle 的数据块可能在内存中也有可能在 HDFS 中，取决于服务器当前的内存是否充足以及对 Hadoop 的配置。如果大小超过一定阈值就写到磁盘，否则放入内存。\\
为了利用SDN controller建立全局的调度策略，优化代码插入位置，在NM获得全局调度信息而不用这样一个一个发送，以提高效率。我们做出了一个初步可用的 HTTP Client POST，向PyCon控制器（SDN controller）POST必要的数据，如：源主机、目的主机、数据长度以及MapID等信息。

\subsection{Python Controller（PyCon）}
\subsubsection{HTTPServer 类}

在 Hadoop 中很多地方都用到了 Servlet，并且使用 Jetty 作为 Servlet 的容器来提供HTTP的服务，其主要是通过 org.apache.hadoop.HTTP.HTTPServer 类实现的，HTTPServer 类是对 Jetty 的简单封装，通过调用 HTTPServer 类的 addServlet 方法增加可以实现增加 Servlet 到 Jetty 的功能。\\
 Hadoop 各个组件都会用到 HTTPServer, datanode/namenode, resourcemanager等。\\
Hadoop 内嵌了 HTTP 服务器 Jetty，主要有以下两方面的作用
\begin{enumerate}
  \item 基于 REST API 的 Web 访问接口，用于展示 Hadoop 的内部状态
  \item 参与 Hadoop 集群的运行和管理
\end{enumerate}

\subsubsection{配置队列，进行限速}
网络研究主要通过两个途径提高QoS：一个是节点控制；另一个是网络控制。节点控制的策略包括流量整形、节点缓冲区管理、队列调度等。网络控制通过对路由的控制来提高网络的性能，其目标是为进入网络的数据流（业务）选择满足其服务质量要求的路径，同时保证网络资源的有效利用。\\
为了提供更好的QoS，我们编写了一个 OVS 队列配置模块来实现最小瓶颈算法，可以在 PyCon 启动时自动完成队列的配置操作，以供算法使用。

\subsubsection{获取网络信息}
采用 Floodlight REST API 获取当前受控制的 SDN 网络的带宽使用信息等。不过，直接调用 REST API 获取到的信息不是直接可用的，而是一个 JSON 格式的文本串。我们编写了一段程序将其自动转化为便于处理的矩阵形式。

\subsubsection{流表预配置}
SDN 网络配置流表相比较常规网络较为缓慢，因为当一个数据包没有匹配的流表，交换机会将数据包发送到控制器来请求得到一个回应，得到回应之后才能继续发送数据包，这个过程的延迟较大，大约是 ms 级别的延迟，这对于常规延迟要求较低的 TCP 网络来说是无法容忍的。在我们实际测试的过程中也发现，如果不进行一个流表的预配置，由于延迟较高，TCP 发送端将认为数据包丢失，会进行数据包重传，且较为严重（大约占到 Shuffle 流量的六分之一）。为了缓解这个问题，我们灵活采用 FL API 获得的拓扑信息对网络进行了分析，在交换机上预配置了一系列的流表。大大缓解了实际 SDN 网络中运行传统应用时的数据包重传问题。

\subsubsection{下发流表}
使用 Floodlight 的 StaticFlowPusher REST API 进行流表的下发。默认超时时间是 5 秒，以避免流表过多匹配效率下降的问题。

\subsubsection{路由算法模块}
我们从修改过的 Hadoop Fetcher 模块和 Floodlight 控制器货物获取即将开始传输的 Flow 的信息，然后调用路径计算模块进行路由规划。目前，PyCon 路径计算模块实现了以下两种路由算法：

\subsubsection{Coflow 调度}
我们从 Hadoop Fetcher 获得了任务的 Job 编号，用于识别 Coflow 信息。然后在采用最小瓶颈算法时将不同的 Coflow 装入不同的队列实现流速控制。但是在实际实现中遇到了一些瓶颈，因此我们先以 MATLAB 仿真结果作为展示。

1.加权最短路径算法\\
思路：先使用 Floodlight 的 API 获得当前整体网络带宽使用信息，然后将返回的信息处理成矩阵，以带宽作为权重，调用 Dijkstra 进行计算，得到一条路径。
\bigskip

2.最小瓶颈算法\\
思路：要使 Flow 的传输时间最短。计算一条流在网络中所有可行的路径，选择一个带宽最大化，传输时间最短的路径，然后进行流表的下发。

细节：
为了实现带宽最大化，我们使用了 OpenFlow 的队列特性。在 Linux 上通过 OVS 调用内核的 QoS Queue 机制实现。队列的配置由 PyCon 完成，不经过 Floodlight。
\subsection{对 Floodlight 的改进}
\subsubsection{Packet-in 数据包处理模块}
Packet-in 数据包是 OpenFlow 协议中当交换机没有对应的匹配流表时，自动向控制器发送的一个数据包，目的是为了获取有效的路由信息，得到控制器的回复之后就可以按照指令传送数据，使数据包可以被顺利发送。

思路：配置流表，让特定数据包以 Packet-in 包的形式传到 Floodlight 控制器。我们编写了一个新模块，使得我们可以顺利提取到 Hadoop Fetcher 无法拿到的端口信息。

具体做法：添加了一个 packet-in 数据包处理模块。可以提取 TCP Source Port 和 Payload 内的 Request URI 并且发送给 PyCon，之所以这么做是因为在 Hadoop MR 模块源码内无法通过 HTTPURLConnection 直接拿到 Source Port
\subsubsection{REST API}
我们使用了 Floodlight 提供的众多开放的 API，比如拓扑信息，Host 信息，网络速率，流表下发与删除等。\\
但是，由于 Floodlight 目前扔在进一步开发中，功能并不完善。我们额外采用了 OVS 的一些特性进行一些特殊操作，比如直接对 OVS 交换机进行流表操作而不经过 Floodlight，这样我们得以实现一些特殊的功能比如下发 actions=normal（Floodlight 无法直接下发之类特殊操作）。
\section{项目的成果}
\subsection{实验仿真测试}
我们经过第五章的一系列的设计开发后，我们开始进行实际的实验测试：

\begin{figure}[!htb]
    \centering
    \let\c@\includegraphics[height=3.5in]{real_topo.png}\relax
    \caption{实验测试网络拓扑结构。此图由 Floodlight 使用生成树算法自动产生，说明拓扑配置无误}
\end{figure}
\begin{figure}[!htb]
    \centering
    \let\c@\includegraphics[height=3.5in]{host_real_flow.png}\relax
    \caption{服务器上的实时流量状况。图为刚执行一个 TeraSort 时的网络流量，可以看到四端口总计高达800Mbps的吞吐量}
\end{figure}
\begin{figure}[!htb]
    \centering
    \let\c@\includegraphics[height=3in]{PyCon_running.png}\relax
    \caption{PyCon的运行情况。调试窗口显示的是 Floodlight 流表下发成功信息}
\end{figure}

\subsection{实验结果}
由于我们主要进行的是 Hadoop 的 Shuffle 网络优化，我们选择了实际应用和测试中网络传输量最大的 TeraSort 进行测试。\\
TeraSort 是 Hadoop MapReduce 上的一个典型的采用 Map-Reduce 架构的范例程序。通常的大规模数据集群中也会采用 1TB（1024GB） 的数据集大小对数据中心性能进行测试。但由于我们的系统规模不大，仅4 台服务器与4台交换机，并且是在新颖的 SDN 架构上对 Hadoop 大数据应用的研究，我们采用的数据集大小仅为 4GB。

实验参数：数据中心内部所有连接速率均为 1 Gbps，关闭了 MapReduce 的 Map 数据压缩选项，生成排序内容时的数据分块大小为 4（对应 4 台 Host）。实际排序时，所使用的 Map Container 的数量为 32 个（视为 32 个分块）。

\begin{figure}[!htb]
    \centering
    \let\c@\includegraphics[height=2.5in]{experiment_result.png}\relax
    \caption{实验测试结果}
\end{figure}

由于我们是在测试 SDN 对整体应用性能的优化效果，我们对 TeraSort 排序的总运行时间进行了测定，从提交任务、分配任务到 Reduce 结果返回结果。实验结果表明，未启用 PyCon 时，完成数据排序的时间平均为 203 秒；启用 PyCon 后，数据排序操作时间平均为 193 秒，约有 5\% 的性能提升。

\subsection{结果分析}
提升性能的因素有以下几点：
缓解了直接套用 SDN 网络而不进行有效配置时 TCP 重传导致的网络拥塞；
Floodlight 默认并不会进行负载均衡操作，而 PyCon 采用的算法集成了负载均衡的功能，使得传输效率大大提升。

性能提升并不显著的原因：
% Hadoop 本身对于常规 TCP 网络的优化工作已经较为到位；
由于测试规模受限，数据集较小，无法显著体现优化的性能；\\
实际数据中心内部的带宽普遍较大（1 Gbps），普遍在 100MB 以内数据块的传输只需要 1 秒以内就可以完成，即使只走 1 条路，也只需要多几秒便可以传输完成；\\
在 Reduce 阶段，有且仅有一个容器可以进行工作，进行各个排序子块数据的合并，导致 Reduce 步骤本身在整个任务运行时间中所占的比重较大，而传输时间是一个小量。

SDN性能瓶颈分析

通过分析sdn网络的工作流程，可知控制器通过响应packet-in消息发送packet-out/flow-mod消息的速度是非常重要的，它的快慢直接影响了控制器拓扑发现，流表下发，mac地址学习能力，甚至整个网络的性能。而且SDN网络中通常采用反应式流安装，控制器的响应时间直接影响着流安装的处理速度，本文将重点测试在负载不同的情况下控制器处理packet-in消息的吞吐量和响应时间。同时也关注控制器支持创建openflow连接的能力与拓扑更新的速度。
(4).虽然我们引用了 Coflow 概念并在系统中进行了实现，但是由于在测试环境中同一时间只运行了一个任务。
\section{项目总结}
在本项目中我们以优化大数据处理性能为背景，将注意力集中到网络性能的优化。最初我们希望基于openstack平台，利用Ryu SDN控制器集中管理数据中心中的虚拟机，来优化大数据处理的性能，基于这个想法，我们提出以下几方面的研究方案：

\begin{enumerate}
  \item 虚拟机的位置优化，动态迁移虚拟机实现负载均衡；
  \item 设计集中控制调控算法，来实现代码传输、数据传输的协同工作；
  \item 设计实时算法来调控网络传输路径，实现网络带宽优化；
  \item 设计实时算法实现数据从硬盘到内存的预加载算法；
  \item 通过用户态协议栈（DPDK）来加速网络处理；
\end{enumerate}
在开发过程中，我们发现这其中的工作量实在太大，正如老师所说，上述几个方案的研究够三个博士毕业的工作了。所以我们将我们的想法收敛，经过组内的讨论和调研，在中期检查后，我们将注意力转到应用层，利用SDN 和云管理工具（Hypervisor）的全局控制能力，设计流级别的调度算法来改善网络性能，于是我们引入了Coflow的概念。另外，由于在openstack平台下，网络配置困难，所以我们考虑先不用虚拟机，而是在直接在服务器上搭建hadoop平台。基于这个平台，我们完成了Coflow调度算法的设计，并在matlab内进行了完整地仿真实验，以及性能的分析。另外在真实系统中，我们实现了利用SDN 集中调控hadoop中的任务流，同时进行了实验测试，在任务的完成时间上我们得到$5\%$的性能提升。

回顾一年来的工作，从最初的认识大数据处理，到平台的选择，算法的设计，仿真、实现、测试以及最后的结果分析总结，我们经历了很多的周折，不断的重新选择，不断的调整方向，最终收敛到目前的工作，感谢这各项目的给予我们的锻炼，收获很多，感谢耐心帮助过我们的师兄师姐，感谢我们的指导老师对我们的帮助和支持。

\bibliographystyle{plain}
\nocite{*}
%\bibliographystyle{unsrt}
\bibliography{references/reference}
%[1] J. Dean et al. MapReduce: Simplified data processing on large clusters. In OSDI, pages 137–150. 2004.\\
%[2] A. D. Ferguson et al. Participatory networking: An API for application control of SDNs. In SIGCOMM. 2013.\\
%[3] Jain S, Kumar A, Mandal S, et al. B4: Experience with a globally-deployed software defined WAN[C] ACM SIGCOMM Computer Communication Review. ACM, 2013, 43(4): 3-14.\\

%\cite{引用文章名称}
%"引用文章名称" 就是前边定义@article后面的名称.


%%%%%%%%%%  参考文献  %%%%%%%%%%

\clearpage
\end{CJK*}                                     % 结束中文字体使用
\end{document}                                 % 结束全文
